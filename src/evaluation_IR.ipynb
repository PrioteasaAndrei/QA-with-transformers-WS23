{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from elasticsearch import Elasticsearch\n",
    "from langchain_community.vectorstores import ElasticsearchStore\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from elasticsearch import Elasticsearch\n",
    "from getpass import getpass\n",
    "from utils import *\n",
    "from dotenv import load_dotenv\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN')\n",
    "HUGGINGFACE_USERNAME = os.getenv('HUGGINGFACE_USERNAME')\n",
    "HUGGINGFACE_DATASET_NAME = os.getenv('HUGGINGFACE_DATASET_NAME')\n",
    "ELASTIC_CLOUD_ID = os.getenv('ELASTIC_CLOUD_ID')\n",
    "ELASTIC_API_KEY = os.getenv('ELASTIC_API_KEY')\n",
    "QA_VALIDATION_DATASET = os.getenv('QA_VALIDATION_DATASET')\n",
    "QA_VALIDATION_TOKEN = os.getenv('QA_VALIDATION_TOKEN')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "model_name = \"NeuML/pubmedbert-base-embeddings\"\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from pprint import pprint\n",
    "\n",
    "# Validation dataset (without RAG answers): https://huggingface.co/datasets/prio7777777/pubmed-qa-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExample on how to run a validation for a given configuration\\nNOTE: this has not been tested holistically, but the code should work\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Example on how to run a validation for a given configuration\n",
    "NOTE: this has not been tested holistically, but the code should work\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mara\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\Mara\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"NeuML/pubmedbert-base-embeddings\"\n",
    "device = 'cuda:0'\n",
    "model_id = \"llama2:latest\" \n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device}\n",
    ")\n",
    "\n",
    "indexes = ['pubmedbert-sentence-transformer-50','pubmedbert-sentence-transformer-100','pubmedbert-sentence-transformer-200','pubmedbert-sentence-transformer-400','pubmedbert-recursive-character-400-overlap-50']\n",
    "\n",
    "## define the LLM model to use | later this can be overwritten by the user\n",
    "# llm = prepare_llm(HUGGINGFACE_TOKEN,model_id=model_id,use_openai=True)\n",
    "llm = Ollama(model = \"llama2:latest\")\n",
    "\n",
    "## create configuration for the run_config function\n",
    "save_path = '../data/chunking_test.csv'\n",
    "save_path_result = \"../data/chunking_test_formatted.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the influence of chunking size and chunk overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_configuration_results = pd.DataFrame(columns=['configuration', 'answer_relevancy', 'context_precision', 'context_recall', 'faithfulness'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_name in indexes:\n",
    "    elastic_vector_search = ElasticsearchStore(\n",
    "        es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "        index_name=index_name,\n",
    "        embedding=embeddings,\n",
    "        es_api_key=ELASTIC_API_KEY,\n",
    "    )\n",
    "\n",
    "    config_1 = {\n",
    "        \"index_name\": index_name,\n",
    "        'evaluation_dataset_path': QA_VALIDATION_DATASET,\n",
    "        'HUGGINGFACE_TOKEN': HUGGINGFACE_TOKEN,\n",
    "        'HUGGINGFACE_DATASET_NAME': HUGGINGFACE_DATASET_NAME,\n",
    "        'llm': llm,\n",
    "        'QA_VALIDATION_TOKEN': QA_VALIDATION_TOKEN,\n",
    "        'save_path': save_path,\n",
    "        'max_retrieved_docs': 3,\n",
    "        'query_transformation_strategy':'read-write-retrieve',\n",
    "        'OPENAI_API_KEY': OPENAI_API_KEY\n",
    "    }\n",
    "\n",
    "    # answers = run_config(elastic_vector_search=elastic_vector_search,\n",
    "    #                  use_ensemble_retriever=False,\n",
    "    #                  verbose=False,\n",
    "    #                  save=True,\n",
    "    #                  **config_1)\n",
    "\n",
    "        \n",
    "    config_2 = {\n",
    "        'QA_VALIDATION_TOKEN': QA_VALIDATION_TOKEN,\n",
    "        'QA_VALIDATION_DATASET': QA_VALIDATION_DATASET,\n",
    "        'save_path': save_path,\n",
    "        'save_path_result': save_path_result, \n",
    "    }\n",
    "\n",
    "    ## this is a Dataset on which the RAGAs metrics can be applied\n",
    "    result_dataset = testset_to_validation(save=True,**config_2)\n",
    "\n",
    "    ## get ragas metrics\n",
    "    resulted_metrics = evaluate(\n",
    "        result_dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    chunking_configuration_results.append({'configuration': index_name, 'answer_relevancy': resulted_metrics['answer_relevancy'], 'context_precision': resulted_metrics['context_precision'], 'context_recall': resulted_metrics['context_recall'], 'faithfulness': resulted_metrics['faithfulness']}, ignore_index=True)\n",
    "\n",
    "    ## save individual results\n",
    "\n",
    "    resulted_metrics.to_pandas().to_csv(f'../data/chunking_configurations/{index_name}_results.csv')\n",
    "\n",
    "\n",
    "## save the results\n",
    "chunking_configuration_results.to_csv('../data/chunking_configurations/chunking_configuration_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 67)\n",
    "config_2 = {\n",
    "        'QA_VALIDATION_TOKEN': QA_VALIDATION_TOKEN,\n",
    "        'QA_VALIDATION_DATASET': QA_VALIDATION_DATASET,\n",
    "        'save_path': save_path,\n",
    "        'save_path_result': save_path_result, \n",
    "    }\n",
    "\n",
    "## this is a Dataset on which the RAGAs metrics can be applied\n",
    "result_dataset = testset_to_validation(save=True,**config_2)\n",
    "result_df = result_dataset.to_pandas()\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get ragas metrics\n",
    "resulted_metrics = evaluate(\n",
    "    result_dataset,\n",
    "    metrics=[\n",
    "        faithfulness,\n",
    "    ],\n",
    ")\n",
    "\n",
    "#faithfulness, PROBLEMATIC\n",
    "# context_precision,\n",
    "#         context_recall,\n",
    "#         answer_relevancy,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## first define embeddings for the db\n",
    "\n",
    "\n",
    "## define what index to use and instantiate the vector store\n",
    "\n",
    "index_name = 'pubmedbert-sentence-transformer-400'\n",
    "\n",
    "elastic_vector_search = ElasticsearchStore(\n",
    "    es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings,\n",
    "    es_api_key=ELASTIC_API_KEY,\n",
    ")\n",
    "\n",
    "## define the LLM model to use | later this can be overwritten by the user\n",
    "llm = prepare_llm(HUGGINGFACE_TOKEN,model_id=model_id,use_openai=True)\n",
    "\n",
    "## create configuration for the run_config function\n",
    "save_path = '../data/rag_validation_answers_400.csv'\n",
    "\n",
    "config_1 = {\n",
    "    \"index_name\": index_name,\n",
    "    'evaluation_dataset_path': QA_VALIDATION_DATASET,\n",
    "    'HUGGINGFACE_TOKEN': HUGGINGFACE_TOKEN,\n",
    "    'HUGGINGFACE_DATASET_NAME': HUGGINGFACE_DATASET_NAME,\n",
    "    'llm': llm,\n",
    "    'QA_VALIDATION_TOKEN': QA_VALIDATION_TOKEN,\n",
    "    'save_path': save_path,\n",
    "    'max_retrieved_docs': 3\n",
    "}\n",
    "\n",
    "## this will save the results under the given path as a csv file\n",
    "## the file will contain the question and the result for each question in the validation dataset (questions generated with RAGas from the new dataset)\n",
    "## takes about 20-30 mins on T4 GPU\n",
    "answers = run_config(elastic_vector_search=elastic_vector_search,\n",
    "                     use_ensemble_retriever=False,\n",
    "                     verbose=True,\n",
    "                     config_name='new_dataset_400',\n",
    "                     save=True,\n",
    "                     **config_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\priot\\anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f455066b8274210a6db0e838f826731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_relevancy': 0.8942793135767543,\n",
      " 'context_precision': 0.9253731342358208,\n",
      " 'context_recall': 0.9253731343283582,\n",
      " 'faithfulness': 0.825542328042328}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_2 = {\n",
    "    'QA_VALIDATION_TOKEN': QA_VALIDATION_TOKEN,\n",
    "    'QA_VALIDATION_DATASET': QA_VALIDATION_DATASET,\n",
    "    'save_path': save_path,\n",
    "    'save_path_result': '../data/validation_400_gpt_3-5-turbo.csv' \n",
    "}\n",
    "\n",
    "## this is a Dataset on which the RAGAs metrics can be applied\n",
    "result_dataset = testset_to_validation(save=True,**config_2)\n",
    "\n",
    "## get ragas metrics\n",
    "resulted_metrics = evaluate(\n",
    "    result_dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],\n",
    ")\n",
    "\n",
    "pprint(resulted_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8942793135767543"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulted_metrics['answer_relevancy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\priot\\anaconda3\\envs\\nlp\\lib\\site-packages\\datasets\\load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228519f811f44951b53a39899c0fcf5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/69698 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
    "\n",
    "loader = HuggingFaceDatasetLoader(\"MaraEliana/pubmed-abstracts\",use_auth_token=\"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\",page_content_column='abstract')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is the role of artificial intelligence in nephrology?\"\n",
    "# results = elastic_vector_search.similarity_search(query,k=50)\n",
    "\n",
    "\n",
    "# titles_elastic = [res.metadata[\"Title\"] for res in results]\n",
    "# for res in results:\n",
    "#     print(res.metadata['Title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Mara\n",
    "\n",
    "Using Ollama generate a bigger validation dataset of 5000 items (make TEST_SET_SIZE 5000).\n",
    "Change ChatOpenAI with the llama model (line 15/16)\n",
    "Save the resulting csv locally and send it to me so I can upload it to huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SET_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ragas \n",
    "from ragas.testset import TestsetGenerator\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from ragas.llms import LangchainLLM\n",
    "import random\n",
    "#https://docs.ragas.io/en/latest/howtos/customisations/llms.html\n",
    "\n",
    "sub_data = random.sample(data, TEST_SET_SIZE)\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "print(OPENAI_API_KEY)\n",
    "\n",
    "# Add custom llms and embeddings\n",
    "generator_llm = LangchainLLM(llm=ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=OPENAI_API_KEY))\n",
    "critic_llm = LangchainLLM(llm=ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=OPENAI_API_KEY)) ## should be gpt-4 but we dont have access\n",
    "embeddings_model = embeddings\n",
    "\n",
    "# Change resulting question type distribution\n",
    "testset_distribution = {\n",
    "    \"simple\": 0.25,\n",
    "    \"reasoning\": 0.25,\n",
    "    \"multi_context\": 0.25,\n",
    "    \"conditional\": 0.25,\n",
    "}\n",
    "\n",
    "# percentage of conversational question\n",
    "chat_qa = 0.1\n",
    "\n",
    "\n",
    "test_generator = TestsetGenerator(\n",
    "    generator_llm=generator_llm,\n",
    "    critic_llm=critic_llm,\n",
    "    embeddings_model=embeddings_model,\n",
    "    testset_distribution=testset_distribution,\n",
    "    chat_qa=chat_qa,\n",
    ")\n",
    "\n",
    "testset = test_generator.generate(sub_data, test_size=TEST_SET_SIZE) ## why second parameter is 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = testset.to_pandas()\n",
    "test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.to_csv('testset.csv',index=False)\n",
    "\n",
    "# index = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "# pprint(test_df.iloc[index]['question'])\n",
    "\n",
    "# ## this is the answer\n",
    "# pprint(test_df.iloc[index]['ground_truth'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## relevant contexts split by \\n\n",
    "# pprint(test_df.iloc[index]['ground_truth_context'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

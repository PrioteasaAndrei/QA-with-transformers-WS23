{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from elasticsearch import Elasticsearch\n",
    "from langchain_community.vectorstores import ElasticsearchStore\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from elasticsearch import Elasticsearch\n",
    "from getpass import getpass\n",
    "from utils import *\n",
    "from dotenv import load_dotenv\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN')\n",
    "HUGGINGFACE_USERNAME = os.getenv('HUGGINGFACE_USERNAME')\n",
    "HUGGINGFACE_DATASET_NAME = os.getenv('HUGGINGFACE_DATASET_NAME')\n",
    "ELASTIC_CLOUD_ID = os.getenv('ELASTIC_CLOUD_ID')\n",
    "ELASTIC_API_KEY = os.getenv('ELASTIC_API_KEY')\n",
    "QA_VALIDATION_DATASET = os.getenv('QA_VALIDATION_DATASET')\n",
    "QA_VALIDATION_TOKEN = os.getenv('QA_VALIDATION_TOKEN')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "model_name = \"NeuML/pubmedbert-base-embeddings\"\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from pprint import pprint\n",
    "\n",
    "# Validation dataset (without RAG answers): https://huggingface.co/datasets/prio7777777/pubmed-qa-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example on how to run a validation for a given configuration\n",
    "NOTE: this has not been tested holistically, but the code should work\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NeuML/pubmedbert-base-embeddings\"\n",
    "device = 'cuda:0'\n",
    "model_id = \"llama2:latest\" \n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device}\n",
    ")\n",
    "\n",
    "indexes = ['pubmedbert-sentence-transformer-50','pubmedbert-sentence-transformer-400','pubmedbert-recursive-character-400-overlap-50']\n",
    "# indexes = ['pubmedbert-sentence-transformer-100']\n",
    "## define the LLM model to use | later this can be overwritten by the user\n",
    "# llm = prepare_llm(HUGGINGFACE_TOKEN,model_id=model_id,use_openai=True)\n",
    "# llm = Ollama(model = \"llama2:latest\")\n",
    "llm = ChatOpenAI(temperature = 0, openai_api_key = OPENAI_API_KEY)\n",
    "\n",
    "## create configuration for the run_config function\n",
    "save_path = '../data/chunking_test.csv'\n",
    "save_path_result = \"../data/chunking_test_formatted.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the influence of chunking size and chunk overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_configuration_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_name in indexes:\n",
    "    elastic_vector_search = ElasticsearchStore(\n",
    "        es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "        index_name=index_name,\n",
    "        embedding=embeddings,\n",
    "        es_api_key=ELASTIC_API_KEY,\n",
    "    )\n",
    "\n",
    "    print(elastic_vector_search.client.info())\n",
    "\n",
    "\n",
    "    config_1 = {\n",
    "        \"index_name\": index_name,\n",
    "        'evaluation_dataset_path': QA_VALIDATION_DATASET,\n",
    "        'HUGGINGFACE_TOKEN': HUGGINGFACE_TOKEN,\n",
    "        'HUGGINGFACE_DATASET_NAME': HUGGINGFACE_DATASET_NAME,\n",
    "        'llm': llm,\n",
    "        'QA_VALIDATION_TOKEN': QA_VALIDATION_TOKEN,\n",
    "        'save_path': save_path,\n",
    "        'max_retrieved_docs': 3,\n",
    "        'OPENAI_API_KEY': OPENAI_API_KEY\n",
    "    }\n",
    "\n",
    "    answers = run_config(elastic_vector_search=elastic_vector_search,\n",
    "                     use_ensemble_retriever=False,\n",
    "                     verbose=False,\n",
    "                     save=True,\n",
    "                     **config_1)\n",
    "\n",
    "        \n",
    "    config_2 = {\n",
    "        'QA_VALIDATION_TOKEN': QA_VALIDATION_TOKEN,\n",
    "        'QA_VALIDATION_DATASET': QA_VALIDATION_DATASET,\n",
    "        'save_path': save_path,\n",
    "        'save_path_result': save_path_result, \n",
    "    }\n",
    "\n",
    "    ## this is a Dataset on which the RAGAs metrics can be applied\n",
    "    result_dataset = testset_to_validation(save=True,**config_2)\n",
    "\n",
    "    ## get ragas metrics\n",
    "    resulted_metrics = evaluate(\n",
    "        result_dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            # faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    chunking_configuration_results.append({'configuration': index_name, 'answer_relevancy': resulted_metrics['answer_relevancy'], 'context_precision': resulted_metrics['context_precision'], 'context_recall': resulted_metrics['context_recall']})\n",
    "\n",
    "    ## save individual results\n",
    "\n",
    "    resulted_metrics.to_pandas().to_csv(f'../data/chunking_configurations/{index_name}_results.csv',index=False)\n",
    "\n",
    "\n",
    "## save the results\n",
    "df = pd.DataFrame(chunking_configuration_results,columns=['configuration', 'answer_relevancy', 'context_precision', 'context_recall'])\n",
    "df.to_csv('../data/chunking_configurations/chunking_configuration_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## first define embeddings for the db\n",
    "\n",
    "\n",
    "## define what index to use and instantiate the vector store\n",
    "\n",
    "index_name = 'pubmedbert-sentence-transformer-400'\n",
    "\n",
    "elastic_vector_search = ElasticsearchStore(\n",
    "    es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings,\n",
    "    es_api_key=ELASTIC_API_KEY,\n",
    ")\n",
    "\n",
    "## define the LLM model to use | later this can be overwritten by the user\n",
    "llm = prepare_llm(HUGGINGFACE_TOKEN,model_id=model_id,use_openai=True)\n",
    "\n",
    "## create configuration for the run_config function\n",
    "save_path = '../data/rag_validation_answers_400.csv'\n",
    "\n",
    "config_1 = {\n",
    "    \"index_name\": index_name,\n",
    "    'evaluation_dataset_path': QA_VALIDATION_DATASET,\n",
    "    'HUGGINGFACE_TOKEN': HUGGINGFACE_TOKEN,\n",
    "    'HUGGINGFACE_DATASET_NAME': HUGGINGFACE_DATASET_NAME,\n",
    "    'llm': llm,\n",
    "    'QA_VALIDATION_TOKEN': QA_VALIDATION_TOKEN,\n",
    "    'save_path': save_path,\n",
    "    'max_retrieved_docs': 3\n",
    "}\n",
    "\n",
    "## this will save the results under the given path as a csv file\n",
    "## the file will contain the question and the result for each question in the validation dataset (questions generated with RAGas from the new dataset)\n",
    "## takes about 20-30 mins on T4 GPU\n",
    "answers = run_config(elastic_vector_search=elastic_vector_search,\n",
    "                     use_ensemble_retriever=False,\n",
    "                     verbose=True,\n",
    "                     config_name='new_dataset_400',\n",
    "                     save=True,\n",
    "                     **config_1)\n",
    "\n",
    "\n",
    "config_2 = {\n",
    "    'QA_VALIDATION_TOKEN': QA_VALIDATION_TOKEN,\n",
    "    'QA_VALIDATION_DATASET': QA_VALIDATION_DATASET,\n",
    "    'save_path': save_path,\n",
    "    'save_path_result': '../data/validation_400_gpt_3-5-turbo.csv' \n",
    "}\n",
    "\n",
    "## this is a Dataset on which the RAGAs metrics can be applied\n",
    "result_dataset = testset_to_validation(save=True,**config_2)\n",
    "\n",
    "## get ragas metrics\n",
    "resulted_metrics = evaluate(\n",
    "    result_dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],\n",
    ")\n",
    "\n",
    "pprint(resulted_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of weight of ensemble retriever in context_precision\n",
    "\n",
    "As we deal with a medical appication we desire precision over recall (in terms of IR).\n",
    "We will analyze this in the context of the weight of the BM25 retriever in the ensemble retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
    "\n",
    "loader = HuggingFaceDatasetLoader(\"MaraEliana/pubmed-abstracts\",use_auth_token=\"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\",page_content_column='abstract')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0, openai_api_key = OPENAI_API_KEY)\n",
    "eval_dataset = load_dataset(QA_VALIDATION_DATASET,token=QA_VALIDATION_TOKEN)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'pubmedbert-sentence-transformer-400'\n",
    "\n",
    "elastic_vector_search = ElasticsearchStore(\n",
    "        es_cloud_id = ELASTIC_CLOUD_ID,\n",
    "        index_name = index_name,\n",
    "        embedding = embeddings,\n",
    "        es_api_key = ELASTIC_API_KEY\n",
    "    )\n",
    "\n",
    "def load_ensemble_retriever(index_name,_elastic_vector_search):\n",
    "    text_splitter = get_splitter_per_index(index_name)\n",
    "    retriever = create_ensemble_retriever(_elastic_vector_search, text_splitter, neuro_weight=0,max_retrieved_docs=20)\n",
    "    return retriever\n",
    "\n",
    "## buffer ensemble retriever for consecutive uses\n",
    "ensemble_retriever = load_ensemble_retriever(index_name,elastic_vector_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        verbose=True,\n",
    "        retriever = ensemble_retriever,\n",
    "        chain_type_kwargs={\n",
    "            \"verbose\": True },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_questions_to_evaluate = 25\n",
    "counter = 0\n",
    "\n",
    "answers = []\n",
    "\n",
    "for example in tqdm(eval_dataset,desc=\"generate RAG answers\"):\n",
    "    answers.append(rag(example['question']))\n",
    "    counter +=1 \n",
    "    if counter == max_questions_to_evaluate:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df = pd.DataFrame(answers)\n",
    "result_df = pd.merge(answers_df, eval_dataset.to_pandas(), left_on='query', right_on='question', how='inner')\n",
    "result_df = result_df.drop(columns=['query','question_type','episode_done'])\n",
    "## first parse the ground_truth and ground_truth context by \\n\n",
    "columns_mapping = {'question': 'question', 'result': 'answer', 'ground_truth_context':'contexts'} #'ground_truth': 'ground_truths',\n",
    "result_df = result_df.rename(columns=columns_mapping)\n",
    "\n",
    "result_df['contexts'] = result_df['contexts'].apply(lambda x: [x])\n",
    "result_df_dataset = Dataset.from_pandas(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulted_metrics = evaluate(\n",
    "    result_df_dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "    ],\n",
    ")\n",
    "\n",
    "pprint(resulted_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of syntethic evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SET_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ragas \n",
    "from ragas.testset import TestsetGenerator\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from ragas.llms import LangchainLLM\n",
    "import random\n",
    "#https://docs.ragas.io/en/latest/howtos/customisations/llms.html\n",
    "\n",
    "sub_data = random.sample(data, TEST_SET_SIZE)\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "print(OPENAI_API_KEY)\n",
    "\n",
    "# Add custom llms and embeddings\n",
    "generator_llm = LangchainLLM(llm=ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=OPENAI_API_KEY))\n",
    "critic_llm = LangchainLLM(llm=ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=OPENAI_API_KEY)) ## should be gpt-4 but we dont have access\n",
    "embeddings_model = embeddings\n",
    "\n",
    "# Change resulting question type distribution\n",
    "testset_distribution = {\n",
    "    \"simple\": 0.25,\n",
    "    \"reasoning\": 0.25,\n",
    "    \"multi_context\": 0.25,\n",
    "    \"conditional\": 0.25,\n",
    "}\n",
    "\n",
    "# percentage of conversational question\n",
    "chat_qa = 0.1\n",
    "\n",
    "\n",
    "test_generator = TestsetGenerator(\n",
    "    generator_llm=generator_llm,\n",
    "    critic_llm=critic_llm,\n",
    "    embeddings_model=embeddings_model,\n",
    "    testset_distribution=testset_distribution,\n",
    "    chat_qa=chat_qa,\n",
    ")\n",
    "\n",
    "testset = test_generator.generate(sub_data, test_size=TEST_SET_SIZE) ## why second parameter is 5?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from opensearch_utils import *\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "HUGGINGFACE_TOKEN = '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'\n",
    "HUGGINGFACE_USERNAME = 'prio7777777'\n",
    "HUGGINGFACE_DATASET_NAME = 'prio7777777/pubmed-demo'\n",
    "vectored_data_path = os.path.join(os.getcwd(),\"pubmed_demo_data.pkl\")\n",
    "\n",
    "connection_settings = {\n",
    "    'DB_USERNAME': 'admin',\n",
    "    'DB_PASSWORD': 'admin',\n",
    "    'DB_HOSTNAME': 'localhost',\n",
    "    'DB_PORT': '9200',\n",
    "}\n",
    "\n",
    "model_name = \"NeuML/pubmedbert-base-embeddings\"\n",
    "## https://huggingface.co/NeuML/pubmedbert-base-embeddings\n",
    "\n",
    "## %python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHyperparams to try:\\n\\n1. number of fragments / sentence fragmentation\\n2. splitting strategy\\n3. model for embeddings\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Hyperparams to try:\n",
    "\n",
    "1. number of fragments / sentence fragmentation\n",
    "2. splitting strategy\n",
    "3. model for embeddings\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.25-cp311-cp311-macosx_10_9_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/site-packages (from langchain) (3.9.1)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.3-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.9 (from langchain)\n",
      "  Downloading langchain_community-0.0.11-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.7 (from langchain)\n",
      "  Downloading langchain_core-0.1.10-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
      "  Downloading langsmith-0.0.80-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/site-packages (from langchain) (1.26.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/site-packages (from langchain) (2.4.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (4.36.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (1.11.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (0.20.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.20.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting anyio<5,>=3 (from langchain-core<0.2,>=0.1.7->langchain)\n",
      "  Downloading anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /usr/local/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.0.3-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/site-packages (from torchvision->sentence-transformers) (10.1.0)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Downloading langchain-0.1.0-py3-none-any.whl (797 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_community-0.0.11-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading langchain_core-0.1.10-py3-none-any.whl (216 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.0.80-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.25-cp311-cp311-macosx_10_9_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Downloading anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.0.3-cp311-cp311-macosx_11_0_universal2.whl (271 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.7/271.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: tenacity, sniffio, mypy-extensions, marshmallow, jsonpointer, greenlet, typing-inspect, SQLAlchemy, jsonpatch, anyio, langsmith, dataclasses-json, langchain-core, langchain-community, langchain\n",
      "Successfully installed SQLAlchemy-2.0.25 anyio-4.2.0 dataclasses-json-0.6.3 greenlet-3.0.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.0 langchain-community-0.0.11 langchain-core-0.1.10 langsmith-0.0.80 marshmallow-3.20.2 mypy-extensions-1.0.0 sniffio-1.3.0 tenacity-8.2.3 typing-inspect-0.9.0\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/site-packages (4.36.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 35.9kB/s]\n",
      "config.json: 100%|██████████| 385/385 [00:00<00:00, 1.16MB/s]\n",
      "vocab.txt: 100%|██████████| 225k/225k [00:00<00:00, 1.20MB/s]\n",
      ".gitattributes: 100%|██████████| 690/690 [00:00<00:00, 1.66MB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 524kB/s]\n",
      "README.md: 100%|██████████| 3.69k/3.69k [00:00<00:00, 10.7MB/s]\n",
      "config.json: 100%|██████████| 629/629 [00:00<00:00, 1.73MB/s]\n",
      "config_sentence_transformers.json: 100%|██████████| 122/122 [00:00<00:00, 215kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:08<00:00, 11.2MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 345kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 398kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.60MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 314/314 [00:00<00:00, 766kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.17MB/s]\n",
      "modules.json: 100%|██████████| 229/229 [00:00<00:00, 561kB/s]\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain sentence-transformers\n",
    "%pip install --upgrade transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "# embed_model_id = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\")\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\")\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 95.0/95.0 [00:00<00:00, 307kB/s]\n",
      "/usr/local/lib/python3.11/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
      "Downloading data: 100%|██████████| 50.8M/50.8M [00:07<00:00, 6.45MB/s]\n",
      "Generating train split: 32584 examples [00:00, 46552.71 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average title length in tokens: 13.918303461821752\n",
      "Average abstract length in tokens: 220.59176282838203\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "if not os.path.exists(vectored_data_path):\n",
    "    dataset = load_dataset(HUGGINGFACE_DATASET_NAME,token=HUGGINGFACE_TOKEN)\n",
    "    ## statistics about the dataset\n",
    "    print(\"Average title length in tokens:\", sum(len(doc['Title'].split()) for doc in dataset['train']) / len(dataset['train']))\n",
    "    print(\"Average abstract length in tokens:\", sum(len(doc['Abstract'].split()) for doc in dataset['train']) / len(dataset['train']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to connect...\n",
      "Connected to OpenSearch {'name': 'opensearch-node1', 'cluster_name': 'opensearch-cluster', 'cluster_uuid': 'vrxH17fESUqSDCdLzzbWxQ', 'version': {'distribution': 'opensearch', 'number': '2.11.0', 'build_type': 'tar', 'build_hash': '4dcad6dd1fd45b6bd91f041a041829c8687278fa', 'build_date': '2023-10-13T02:55:55.511945994Z', 'build_snapshot': False, 'lucene_version': '9.7.0', 'minimum_wire_compatibility_version': '7.10.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'The OpenSearch Project: https://opensearch.org/'}\n",
      "Creating index...\n",
      "Successfully created index\n"
     ]
    }
   ],
   "source": [
    "\n",
    "database_connection = opensearch_connection('pubmed-index',connection_settings=connection_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tranformers_token_text_splitter = splitter = SentenceTransformersTokenTextSplitter(\n",
    "            model_name=model_name,  # specify the model used for tokenization\n",
    "            chunk_overlap=10,  # set the overlap between consecutive text chunks\n",
    "        )\n",
    "\n",
    "spacy_splitter = SpacyTextSplitter()\n",
    "\n",
    "# def get_chunking(text,type='SentenceTransformersTokenTextSplitter'):\n",
    "#     if type=='SentenceTransformersTokenTextSplitter':\n",
    "#         # create a SentenceTransformersTokenTextSplitter object\n",
    "        \n",
    "#         return sentence_tranformers_token_text_splitter.split_text(text)\n",
    "#     elif type=='SpacyTextSplitter':\n",
    "#         ## TODO: this only works because the abstracts are short so we only gen a list with one element not chunks\n",
    "#         ## the returned list from splitter.split_text(text) is a list of a single element of sentences divided by \\n\\n\n",
    "#         return spacy_splitter.split_text(text)[0].split(\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectored_data = []\n",
    "if not os.path.exists(vectored_data_path):\n",
    "\n",
    "    for item in tqdm(dataset['train']):\n",
    "        title = item['Title']\n",
    "        abstract = item['Abstract'].replace(\"\\n\",\" \")\n",
    "        date = item['Published_date'] ## may be NaN\n",
    "        if date is None:\n",
    "            date = \"no published date\"\n",
    "        authors = item['Authors'].lower()\n",
    "        journal = item['Journal'].lower()\n",
    "        authors_info = item['Author_Information'].lower()\n",
    "\n",
    "        ## most probably this doesnt work\n",
    "        ## investigate the chunking with spacy\n",
    "        ## done like this for performance reasons not interpretability\n",
    "        \n",
    "        chunks = sentence_tranformers_token_text_splitter.split_text(abstract)\n",
    "\n",
    "        # chunks = spacy_splitter.split_text(abstract)[0].split(\"\\n\\n\")\n",
    "\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            metadata = {\n",
    "                \"title\": title,\n",
    "                \"chunk_id\": j,\n",
    "                \"chunk_text\": chunk,\n",
    "                \"date\": date,\n",
    "                \"authors\": authors,\n",
    "                \"journal\": journal,\n",
    "                \"authors_info\": authors_info\n",
    "            }\n",
    "\n",
    "            embedding = model.encode(chunk).tolist()\n",
    "\n",
    "            ##TODO: create unique identifier\n",
    "            \n",
    "            vectored_data.append((metadata, embedding))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to store the list\n",
    "def store_list(data, filename):\n",
    "  \"\"\"\n",
    "  Stores a list of tuples containing (id, embedding, metadata) to a file.\n",
    "\n",
    "  Args:\n",
    "      data: A list of tuples containing (id, embedding, metadata).\n",
    "      filename: The filename to store the data.\n",
    "  \"\"\"\n",
    "  with open(filename, \"wb\") as f:\n",
    "      # use pickle to serialize the data\n",
    "      import pickle\n",
    "      pickle.dump(data, f)\n",
    "\n",
    "\n",
    "# define a function to read the list\n",
    "def read_list(filename):\n",
    "  \"\"\"\n",
    "  Reads a list of tuples containing (embedding, metadata) from a file.\n",
    "\n",
    "  Args:\n",
    "      filename: The filename to read the data from.\n",
    "\n",
    "  Returns:\n",
    "      A list of tuples containing (id, embedding, metadata).\n",
    "  \"\"\"\n",
    "  with open(filename, \"rb\") as f:\n",
    "      # Use pickle to deserialize the data\n",
    "      import pickle\n",
    "      data = pickle.load(f)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(vectored_data_path):\n",
    "    store_list(vectored_data, \"pubmed_demo_data.pkl\")\n",
    "\n",
    "vectored_data = read_list(\"pubmed_demo_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32822, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## convert embedding to numpy array\n",
    "import numpy as np\n",
    "embeddings = np.array([item[1] for item in vectored_data])\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving articles to database: 100%|██████████| 32822/32822 [37:52<00:00, 14.45it/s]  \n"
     ]
    }
   ],
   "source": [
    "loadArticlesVector(database_connection,vectored_data,'pubmed-index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8097, 0.4761, 0.8706, 0.1207])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### sanity STS check\n",
    "query = \"The algorithm named GasMIL was established and demonstrated encouraging performance in diagnosing IM AUC 0884\"\n",
    "\n",
    "documents = ['GasMIL proved good performance in diagnosing AUC 0884', ## big\n",
    "             'The algorithm showed really interesting results for diagnosing purposes', ## big\n",
    "             'GasMIL is a new algorithm for diagnosing IM AUC 0884', ## medium\n",
    "             'The dog is walking out in the park'] ## small \n",
    "\n",
    "# encode the query and the documents\n",
    "query_embedding = model.encode(query)\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "# compute the cosine similarity scores\n",
    "cos_scores = util.pytorch_cos_sim(query_embedding, document_embeddings)[0]\n",
    "\n",
    "cos_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['What is CV used for in medicine?']\n",
    "query_vector = model.encode(query)\n",
    "query_embedding = query_vector.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the OpenSearch search body\n",
    "body = {\n",
    "    # **Query:** match all documents and score them based on a custom script\n",
    "    \"query\": {\n",
    "        \"script_score\": {\n",
    "            # match all documents\n",
    "            \"query\": {\n",
    "                \"match_all\": {}\n",
    "            },\n",
    "            # define a script to calculate the score\n",
    "            \"script\": {\n",
    "                # since cosine similarity ranges between -1 and 1 and\n",
    "                # opensearch is not able to process negative cosine similarity score\n",
    "                # therefore +1.0 is added\n",
    "                \"source\": \"cosineSimilarity(params.queryVector, doc['vector']) + 1.0\",\n",
    "                # pass the query vector as a parameter to the script\n",
    "                \"params\": {\n",
    "                    \"queryVector\": query_embedding\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    # filter results with a minimum score of 1.45\n",
    "    \"min_score\": 1.40\n",
    "}\n",
    "\n",
    "# set the maximum number of results to retrieve\n",
    "size = 1000\n",
    "\n",
    "# perform the search with a 120-second timeout\n",
    "aux_results = database_connection.search(\n",
    "    index='pubmed-index',\n",
    "    body=body,\n",
    "    size=size,\n",
    "    request_timeout=120\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "as interest in ai in medicine grows so too does the need for education on the topic despite the technology itself being so close our understanding of the essence of how it works remains remote a greater more judicious acceptance of ai tools can be fostered in medicine by a broader appreciation of what the technology can and cannot do\n",
      "Score: 1.4744328\n",
      "Title: Artificial Intelligence or just statistics done different\n",
      "----------\n",
      "image data has grown exponentially as systems have increased their ability to collect and store it unfortunately there are limits to human resources both in time and knowledge to fully interpret and manage that data computer vision cv has grown in popularity as a discipline for better understanding visual data computer vision has become a powerful tool for imaging analytics in orthopedic surgery allowing computers to evaluate large volumes of image data with greater nuance than previously possible nevertheless even with the growing number of uses in medicine literature on the fundamentals of cv and its implementation is mainly oriented toward computer scientists rather than clinicians rendering cv unapproachable for most orthopedic surgeons as a tool for clinical practice and research the purpose of this article is to summarize and review the fundamental concepts of cv application for the orthopedic surgeon and musculoskeletal researcher\n",
      "Score: 1.4536703\n",
      "Title: Educational Overview of the Concept and Application of Computer Vision in Arthroplasty\n",
      "----------\n",
      "aims to provide an overview of the role of cardiovascular cv imaging in facilitating and advancing the field of precision medicine in cv disease methods and results non invasive cv imaging is essential to accurately and efficiently phenotype patients with heart disease including coronary artery disease cad and heart failure hf various modalities such as echocardiography nuclear cardiology cardiac computed tomography ct cardiovascular magnetic resonance cmr and invasive coronary angiography and in some cases a combination can be required to provide sufficient information for diagnosis and management taking cad as an example imaging is essential for the detection and functional assessment of coronary stenoses as well as for the quantification of cardiac function and ischaemic myocardial damage furthermore imaging may detect and quantify coronary atherosclerosis potentially identify plaques at increased risk of rupture and guide coronary interventions in patients with hf imaging helps identify specific aetiologies quantify damage and assess its impact on cardiac function imaging plays a central role in individualizing diagnosis and management and to determine the optimal treatment for each patient to increase the likelihood of response and improve patient outcomes conclusions advances in all imaging techniques continue to improve accuracy sensitivity and standardization of functional and prognostic assessments and identify established and novel therapeutic targets combining imaging with artificial intelligence machine learning and computer algorithms as well as with genomic transcriptomic proteomic and metabolomic approaches will become state of the art in the future to understand pathologies of cad and hf and in the development of new targeted therapies\n",
      "Score: 1.4474739\n",
      "Title: Non invasive imaging as the cornerstone of cardiovascular precision medicine\n",
      "----------\n",
      "a busy community cardiologist finished reading eight echocardiograms over lunch and started clinic at 1pm as three patients waited jane a 45 year old graphic designer was seen for skipped heart beat she works about 50h a week exercises at the local gym and enjoys eating a healthy diet about 4months ago jane began experiencing her heart skipping beats she initially attributed the symptoms to long hours in the office and caffeine but over the holiday her brother purchased a smart watch and she began digitally recording her cardiac rhythm about a month ago the device detected possible atrial fibrillation so she called and scheduled this visit for a cardiology consultation upon that visitation she and her physician reviewed the device readings while it appeared to be an irregular rhythm before either considered a treatment plan they began to ask questions ranging from the following is this an accurate diagnosis what other data are available to better understand the risk of a cardiac arrhythmia how is this data analyzed so that the best treatment plan can be made and what type of clinical decision support system is required to virtually monitor people like me using digital health devices to improve the efficiency and quality of care delivered in population health\n",
      "Score: 1.4466594\n",
      "Title: Virtual Care 2 0 a Vision for the Future of Data Driven Technology Enabled Healthcare\n",
      "----------\n",
      "science fiction sf is ubiquitous and it has also been utilised for the purposes of teaching since it has replaced legend myth and fable this is especially the case for star trek st which has become an integral part of popular culture even for those who do not follow sf in this best practice guideline bpg we will engage topics that ordinary readers of ehd might not normally come across but may well find interesting we will review the individual doctors in st from the viewpoint of a medical doctor and will demonstrate the ways in which the medic in the various series which spans decades since 1966 reflects the zeitgeist a second bpg will provide an assortment of st cautionary tales which range from nanotechnology to the holocaust to artificial intelligence sf is famously extravagant fiction today cold fact tomorrow and takes us where no man has gone before the imaginings of sf authors some of whom are scientists and doctors should be taken seriously for potential detrimental effects events that may befall the medical profession or the human race and that might be avoided with the foresight provided by sf\n",
      "Score: 1.4395014\n",
      "Title: Editorial\n",
      "----------\n",
      "prove that it can perform at a level comparable to if not better than humans jvs vascular insights 2023 1 100019\n",
      "Score: 1.4282686\n",
      "Title: The potential of chatbots in chronic venous disease patient management\n",
      "----------\n",
      "looking at the extremely large amount of literature as summarized in two recent reviews on applications of artificial intelligence in cardiology both in the adult and pediatric age groups published in the journal of clinical medicine\n",
      "Score: 1.4168578\n",
      "Title: Artificial Intelligence in Cardiology Why So Many Great Promises and Expectations but Still a Limited Clinical Impact\n",
      "----------\n",
      "the search for new strategies for better understanding cardiovascular cv disease is a constant one spanning multitudinous types of observations and studies a comprehensive characterization of each disease state and its biomolecular underpinnings relies upon insights gleaned from extensive information collection of various types of data researchers and clinicians in cv biomedicine repeatedly face questions regarding which types of data may best answer their questions how to integrate information from multiple datasets of various types and how to adapt emerging advances in machine learning and or artificial intelligence to their needs in data processing frequently lauded as a field with great practical and translational potential the interface between biomedical informatics and cv medicine is challenged with staggeringly massive datasets successful application of computational approaches to decode these complex and gigantic amounts of information becomes an essential step toward realizing the desired benefits in this review we examine recent efforts to adapt informatics strategies to cv biomedical research automated information extraction and unification of multifaceted omics data we discuss how and why this interdisciplinary space of cv informatics is particularly relevant to and supportive of current experimental and clinical research we describe in detail how open data sources and methods can drive discovery while demanding few initial resources an advantage afforded by widespread availability of cloud computing driven platforms subsequently we provide examples of how interoperable computational systems facilitate exploration of data from multiple sources including both consistently formatted structured data and unstructured data taken together these approaches for achieving data harmony enable molecular phenotyping of cv diseases and unification of cv knowledge\n",
      "Score: 1.41675\n",
      "Title: Cardiovascular informatics building a bridge to data harmony\n",
      "----------\n",
      "with progress in information and communication technology medical information has been converted to digital formats and stored and managed using computer systems the construction management and operation of medical information systems and regional medical liaison systems are the main components of the clinical tasks of medical informatics departments research using medical information accumulated in these systems is also a task for medical informatics department recently medical real world data rwd accumulated in medical information systems has become a focus not only for primary use but also for secondary uses of medical information however there are many problems such as standardization collection cleaning and analysis of them the internet of things and artificial intelligence are also being applied in the collection and analysis of rwd and in resolving the above problems using these new technologies progress in medical care and clinical research is about to enter a new era j med invest 67 27 29 february 2020\n",
      "Score: 1.4131246\n",
      "Title: The role of medical informatics in the management of medical information\n",
      "----------\n",
      "artificial intelligence is new and enticing but the idea that it will make physicians expendable is hyperbole medicine is a moral art that balances values and desires old fashioned human intelligence will always be needed in medicine\n",
      "Score: 1.4080386\n",
      "Title: Old fashioned Intelligence Will Always Be Needed in Medicine\n",
      "----------\n",
      "mathematical sciences have had a huge development with the use of numbers which is used to process images sounds and computer languages medical knowledge is collected in databases in text format groups of words so far words in medicine have never been processed and we are not currently able to create connections between them in any way with mathematical logic words can be treated as numbers words using logical connectives become more and more calculable and developable with the support of mathematical and computer sciences the words in medicine may have the same development of the numbers in mathematical sciences words that belong to the history of the patient physical examinations and clinical data can be gathered in tables therefore they can be made available to computer applications creating a digital memory by presenting it as required by the doctor the author believes that the clinical reasoning of the doctor uses connectives available in mathematical logic therefore thought can be supported by mathematical calculation knowledge engineering programs will develop data return or self generated algorithms up to the future use of artificial intelligence in the field of nephrology\n",
      "Score: 1.4059745\n",
      "Title: Nephrology knowledge and digital memory\n",
      "----------\n",
      "extracting medical knowledge by structured data mining of many medical records and from unstructured data mining of natural language source text on the internet will become increasingly important for clinical decision support output from these sources can be transformed into large numbers of elements of knowledge in a knowledge representation store krs here using the notation and to some extent the algebraic principles of the q uel web based universal exchange and inference language described previously rooted in dirac notation from quantum mechanics and linguistic theory in a krs semantic structures or statements about the world of interest to medicine are analogous to natural language sentences seen as formed from noun phrases separated by verbs prepositions and other descriptions of relationships a convenient method of testing and better curating these elements of knowledge is by having the computer use them to take the test of a multiple choice medical licensing examination it is a venture which perhaps tells us almost as much about the reasoning of students and examiners as it does about the requirements for artificial intelligence as employed in clinical decision making it emphasizes the role of context and of contextual probabilities as opposed to the more familiar intrinsic probabilities and of a preliminary form of logic that we call presyllogistic reasoning\n",
      "Score: 1.4059159\n",
      "Title: Data mining to build a knowledge representation store for clinical decision support Studies on curation and validation based on machine performance in multiple choice medical licensing examinations\n",
      "----------\n",
      "we present here a vision of individualized knowledge graphs ikgs in cardiovascular medicine a modern informatics platform of exchange and inquiry that comprehensively integrates biological knowledge with medical histories and health outcomes of individual patients we envision that this could transform how clinicians and scientists together discover communicate and apply new knowledge\n",
      "Score: 1.4047456\n",
      "Title: Individualized Knowledge Graph A Viable Informatics Path to Precision Medicine\n",
      "----------\n",
      "the current state of the development of medicine today is changing dramatically previously data of the patient s health were collected only during a visit to the clinic these were small chunks of information obtained from observations or experimental studies by clinicians and were recorded on paper or in small electronic files the advances in computer power development hardware and software tools and consequently design an emergence of miniature smart devices for various purposes flexible electronic devices medical tattoos stick on sensors biochips etc can monitor various vital signs of patients in real time and collect such data comprehensively there is a steady growth of such technologies in various fields of medicine for disease prevention diagnosis and therapy due to this clinicians began to face similar problems as data scientists they need to perform many different tasks which are based on a huge amount of data in some cases with incompleteness and uncertainty and in most others with complex non obvious connections between them and different for each individual patient observation as well as a lack of time to solve them effectively these factors significantly decrease the quality of decision making which usually affects the effectiveness of diagnosis or therapy that is why the new concept in medicine widely known as data driven medicine arises nowadays this approach which based on iot and artificial intelligence provide possibilities for efficiently process of the huge amounts of data of various types stimulates new discoveries and provides the necessary integration and management of such information for enabling precision medical care such approach could create a new wave in health care it will provide effective management of a huge amount of comprehensive information about the patient s condition will increase the speed of clinician s expertise and will maintain high accuracy analysis based on digital tools and machine learning the combined use of different digital devices and artificial intelligence tools will provide an opportunity to deeply understand the disease boost the accuracy and speed of its detection at early stages and improve the modes of diagnosis such invaluable information stimulates new ways to choose patient oriented preventions and interventions for each individual case\n",
      "Score: 1.4018521\n",
      "Title: Special issue Informatics data driven medicine\n",
      "----------\n",
      "the history of medicine is punctuated by conquests discoveries and revolutions it is also marked by questioning it is made of doubts and certainties in this thousand years old history certain recent battles bear witness to these questionings such as quality refocusing on the patient medical errors antibiotic resistance and the importance of gender which has been neglected for so long in medicine digitalization is one of these many revolutions and it is not immune to questioning building evidence and trust equity of access for neglected populations and training are among these issues more specifically in the field of decision support the first enthusiastic hours of computing were followed by unexpected observations such as the identification of human factors such as alert fatigue today immense hopes rest on the development of deep learning and it is up to us to accelerate its development by investing energy time and resources to build on evidence trust and a strong integration of health professionals and patients\n",
      "Score: 1.4009894\n",
      "Title: Health Digital Health and Decision Support Sisyphus and Pandora\n",
      "----------\n",
      "the history of drug safety monitoring or pharmacovigilance has been an interesting one despite many and ongoing changes it has typically been characterized by a rather slow moving and reactive progression pharmacovigilance has always lagged behind other fields and industries and has been slow to adapt to new approaches the main aspect holding it back has been a focus on the administrative and adherence side of creating individual case safety reports icsrs and distributing these reports to the various stakeholders per strict regulatory requirements now in 2018 we are more behind the curve than ever and the field seems to be at a breaking point calling for urgent and drastic changes the question at hand is whether in this era of an abundance of electronically available data and technological advancements which allow the application of automation this process still makes sense is there still a place for creating and redistributing icsrs from marketed use in a current state of the art safety system artificial intelligence deep machine learning and related technologies are already in place in many other industries swift and rigorous change is necessary for the discipline of pharmacovigilance to keep up with what is happening in the world at large\n",
      "Score: 1.400158\n",
      "Title: Why Are We Still Creating Individual Case Safety Reports\n"
     ]
    }
   ],
   "source": [
    "# loop over each returned hit in the search results\n",
    "for result in aux_results[\"hits\"][\"hits\"]:\n",
    "    # print a separator for each result\n",
    "    print(\"-\" * 10)\n",
    "    print(result['_source']['pubmed_text'])\n",
    "    # print the score of the document\n",
    "    print(f\"Score: {result['_score']}\")\n",
    "    # print the title of the document stored in the \"_source\" field\n",
    "    print(f\"Title: {result['_source']['title']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
